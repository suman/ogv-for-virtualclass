(function webpackUniversalModuleDefinition(root, factory) {
	if(typeof exports === 'object' && typeof module === 'object')
		module.exports = factory();
	else if(typeof define === 'function' && define.amd)
		define([], factory);
	else if(typeof exports === 'object')
		exports["AudioFeeder"] = factory();
	else
		root["AudioFeeder"] = factory();
})(this, function() {
return /******/ (function(modules) { // webpackBootstrap
/******/ 	// The module cache
/******/ 	var installedModules = {};

/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {

/******/ 		// Check if module is in cache
/******/ 		if(installedModules[moduleId])
/******/ 			return installedModules[moduleId].exports;

/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = installedModules[moduleId] = {
/******/ 			exports: {},
/******/ 			id: moduleId,
/******/ 			loaded: false
/******/ 		};

/******/ 		// Execute the module function
/******/ 		modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);

/******/ 		// Flag the module as loaded
/******/ 		module.loaded = true;

/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}


/******/ 	// expose the modules object (__webpack_modules__)
/******/ 	__webpack_require__.m = modules;

/******/ 	// expose the module cache
/******/ 	__webpack_require__.c = installedModules;

/******/ 	// __webpack_public_path__
/******/ 	__webpack_require__.p = "";

/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(0);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports, __webpack_require__) {

	(function() {

		var BufferQueue = __webpack_require__(1),
			WebAudioBackend = __webpack_require__(2),
			FlashBackend = __webpack_require__(4),
			AudioTempoChanger = __webpack_require__(6);


		/**
		 * Audio sample buffer format passed to {@link AudioFeeder#bufferData} and its backends.
		 *
		 * Buffers are arrays containing one Float32Array of sample data
		 * per channel. Channel counts must match the expected value, and
		 * all channels within a buffer must have the same length in samples.
		 *
		 * Since input data may be stored for a while before being taken
		 * back out, be sure that your Float32Arrays for channel data are
		 * standalone, not backed on an ArrayBuffer that might change!
		 *
		 * @typedef {SampleBuffer} SampleBuffer
		 * @todo consider replacing this with AudioBuffer-like wrapper object
		 */

		/**
		 * Object dictionary format used to pass options into {@link AudioFeeder} and its backends.
		 *
		 * @typedef {Object} AudioFeederOptions
		 * @property {string} base - (optional) base URL to find additional resources in,
		 *                           such as the Flash audio output shim
		 * @property {AudioContext} audioContext - (optional) Web Audio API AudioContext
		 *                          instance to use inplace of creating a default one
		 */

		/**
		 * Object dictionary format returned from {@link AudioFeeder#getPlaybackState} and friends.
		 *
		 * @typedef {Object} PlaybackState
		 * @property {number} playbackPosition - total seconds so far of input audio data that have played (pre tempo change)
		 * @property {number} outputPlaybackPosition - total seconds so far of audio that has been played (post tempo change)
		 * @property {number} samplesQueued - number of samples at target rate that are queued up for playback
		 * @property {number} dropped - number of underrun events, when we had to play silence due to data starvation
		 * @property {number} delayedTime - total seconds so far of silent time during playback due to data starvation
		 * @todo drop 'dropped' in favor of delayedTime
		 * @todo replace sampledQueued with a time unit?
		 */

		/**
		 * Object that we can throw audio data into and have it drain out.
		 * @class
		 * @param {AudioFeederOptions} options - dictionary of config settings
		 *
		 * @classdesc
		 * Object that we can throw audio data into and have it drain out.
		 */
		function AudioFeeder(options) {
			this._options = options || {};
			this._backend = null; // AudioBackend instance, after init...
			this._resampleFractional = 0;
			this._resampleLastSampleData = undefined;
			this._tempoChanger = null; // Initialized at init
		};

		/**
		 * Sample rate in Hz, as requested by the caller in {@link AudioFeeder#init}.
		 *
		 * If the backend's actual sample rate differs from this requested rate,
		 * input data will be resampled automatically.
		 *
		 * @type {number}
		 * @readonly
		 * @see AudioFeeder#targetRate
		 */
		AudioFeeder.prototype.rate = 0;

		/**
		 * Actual output sample rate in Hz, as provided by the backend.
		 * This may differ from the rate requested, in which case input data
		 * will be resampled automatically.
		 *
		 * @type {number}
		 * @readonly
		 * @see AudioFeeder#rate
		 */
		AudioFeeder.prototype.targetRate = 0;

		/**
		 * Number of output channels, as requested by the caller in {@link AudioFeeder#init}.
		 *
		 * If the backend's actual channel count differs from this requested count,
		 * input data will be resampled automatically.
		 *
		 * Warning: currently more than 2 channels may result in additional channels
		 * being cropped out, as downmixing has not yet been implemented.
		 *
		 * @type {number}
		 * @readonly
		 */
		AudioFeeder.prototype.channels = 0;

		/**
		 * Size of output buffers in samples, as a hint for latency/scheduling
		 * @type {number}
		 * @readonly
		 */
		AudioFeeder.prototype.bufferSize = 0;

		/**
		 * Duration of the minimum buffer size, in seconds.
		 * If the amount of buffered data falls below this,
		 * caller will receive a synchronous 'starved' event
		 * with a last chance to buffer more data.
		 *
		 * @type {number}
		 * @readonly
		 */
		Object.defineProperty(AudioFeeder.prototype, 'bufferDuration', {
			get: function getBufferDuration() {
				if (this.targetRate) {
					return this.bufferSize / this.targetRate;
				} else {
					return 0;
				}
			}
		});

		/**
		 * Duration of remaining data at which a 'bufferlow' event will be
		 * triggered, in seconds.
		 *
		 * This defaults to twice bufferDuration, but can be overridden.
		 *
		 * @type {number}
		 */
		Object.defineProperty(AudioFeeder.prototype, 'bufferThreshold', {
			get: function getBufferThreshold() {
				if (this._backend) {
					return this._backend.bufferThreshold / this.targetRate;
				} else {
					return 0;
				}
			},
			set: function setBufferThreshold(val) {
				if (this._backend) {
					this._backend.bufferThreshold = Math.round(val * this.targetRate);
				} else {
					throw 'Invalid state: AudioFeeder cannot set bufferThreshold before init';
				}
			}
		});


		/**
		 * Current playback position, in seconds, in input time (i.e. pre tempo change)
		 * This compensates for drops and underruns, and is suitable for a/v sync.
		 *
		 * @type {number}
		 * @readonly
		 */
		Object.defineProperty(AudioFeeder.prototype, 'playbackPosition', {
			get: function getPlaybackPosition() {
				if (this._backend) {
					var playbackState = this.getPlaybackState();
					return playbackState.playbackPosition;
				} else {
					return 0;
				}
			}
		});

		/**
		 * Current playback position, in seconds, in output time (i.e. post tempo change)
		 * Also compensates for drops and underruns, and is suitable for a/v sync.
		 *
		 * @type {number}
		 * @readonly
		 */
		Object.defineProperty(AudioFeeder.prototype, 'outputPlaybackPosition', {
			get: function getOutputPlaybackPosition() {
				if (this._backend) {
					var playbackState = this.getPlaybackState();
					return playbackState.outputPlaybackPosition;
				} else {
					return 0;
				}
			}
		});

		/**
		 * Duration of remaining queued data, in seconds.
		 *
		 * @type {number}
		 */
		Object.defineProperty(AudioFeeder.prototype, 'durationBuffered', {
			get: function getDurationBuffered() {
				if (this._backend) {
					var playbackState = this.getPlaybackState();
					return playbackState.samplesQueued / this.targetRate;
				} else {
					return 0;
				}
			}
		});

		/**
		 * Is the feeder currently set to mute output?
		 * When muted, this overrides the volume property.
		 *
		 * @type {boolean}
		 */
		Object.defineProperty(AudioFeeder.prototype, 'muted', {
	 		get: function getMuted() {
				if (this._backend) {
					return this._backend.muted;
				} else {
					throw 'Invalid state: cannot get mute before init';
				}
	 		},
	 		set: function setMuted(val) {
				if (this._backend) {
					this._backend.muted = val;
				} else {
					throw 'Invalid state: cannot set mute before init';
				}
	 		}
	 	});

		/**
		 * @deprecated in favor of muted and volume properties
		 */
		AudioFeeder.prototype.mute = function() {
			this.muted = true;
		};

		/**
		* @deprecated in favor of muted and volume properties
		 */
		AudioFeeder.prototype.unmute = function() {
			this.muted = false;
		};

		/**
		 * Volume multiplier, defaults to 1.0.
		 * @name volume
		 * @type {number}
		 */
		Object.defineProperty(AudioFeeder.prototype, 'volume', {
			get: function getVolume() {
				if (this._backend) {
					return this._backend.volume;
				} else {
					throw 'Invalid state: cannot get volume before init';
				}
			},
			set: function setVolume(val) {
				if (this._backend) {
					this._backend.volume = val;
				} else {
					throw 'Invalid state: cannot set volume before init';
				}
			}
		});

		/**
		 * Tempo multiplier, defaults to 1.0.
		 * @name volume
		 * @type {number}
		 */
		Object.defineProperty(AudioFeeder.prototype, 'tempo', {
			get: function getTempo() {
				if (this._tempoChanger) {
					return this._tempoChanger.getTempo();
				} else {
					throw 'Invalid state: cannot get tempo before init';
				}
			},
			set: function setTempo(val) {
				if (this._tempoChanger) {
					this._tempoChanger.setTempo(val);
				} else {
					throw 'Invalid state: cannot set tempo before init';
				}
			}
		});


		/**
		 * Start setting up for output with the given channel count and sample rate.
		 * Audio data you provide will be resampled if necessary to whatever the
		 * backend actually supports.
		 *
		 * @param {number} numChannels - requested number of channels (output may differ)
		 * @param {number} sampleRate - requested sample rate in Hz (output may differ)
		 *
		 * @todo merge into constructor?
		 */
		AudioFeeder.prototype.init = function(numChannels, sampleRate) {
			this.channels = numChannels;
			this.rate = sampleRate;

			if (WebAudioBackend.isSupported()) {
				this._backend = new WebAudioBackend(numChannels, sampleRate, this._options);
			} else if (FlashBackend.isSupported()) {
				this._backend = new FlashBackend(numChannels, sampleRate, this._options);
			} else {
				throw 'No supported backend';
			}

			this.targetRate = this._backend.rate;
			this.bufferSize = this._backend.bufferSize;

			// Initialize tempo changer (NB! no "new")
			this._tempoChanger = AudioTempoChanger({ sampleRate: this.targetRate, numChannels: numChannels, tempo: 1.0 });

			// Pass through the starved event
			this._backend.onstarved = (function() {
				if (this.onstarved) {
					this.onstarved();
				}
			}).bind(this);
			this._backend.onbufferlow = (function() {
				if (this.onbufferlow) {
					this.onbufferlow();
				}
			}).bind(this);
		};

		/**
		 * Resample a buffer from the input rate/channel count to the output.
		 *
		 * This is horribly naive and wrong.
		 * Replace me with a better algo!
		 *
		 * @param {SampleBuffer} sampleData - input data in requested sample rate / channel count
		 * @returns {SampleBuffer} output data in backend's sample rate/channel count
		 */
		AudioFeeder.prototype._resample = function(sampleData) {
			var rate = this.rate,
				channels = this.channels,
				targetRate = this._backend.rate,
				targetChannels = this._backend.channels;

			if (rate == targetRate && channels == targetChannels) {
				return sampleData;
			} else {
				var newSamples = [];

				// Mind that packet boundaries won't always align on
				// sample boundaries in the resamples output, so maintain
				// a running rounding fractional offset of the portion of
				// a sample we'll have to pull from the previous run on
				// the next one.
				var inputLen = sampleData[0].length,
					previousFractional = this._resampleFractional,
					outputLen = inputLen * targetRate / rate + previousFractional,
					outputSamples = Math.floor(outputLen),
					remainingFractional = (outputLen - outputSamples);

				var interpolate;
				if (rate < targetRate) {
					// Input rate is lower than the target rate,
					// use linear interpolation to minimize "tinny" artifacts.
					interpolate = function(input, output, previous, adjustment) {
						var inputSample = function(i) {
							if (i < 0) {
								if (previous && previous.length + i > 0) {
									// Beware sometimes we have empty bits at start.
									return previous[previous.length + i];
								} else {
									// this probably shouldn't happen
									// but if it does be safe ;)
									return input[0];
								}
							} else {
								return input[i];
							}
						};

						for (var i = 0; i < output.length; i++) {
							// Map the output sample to input space,
							// offset by one to give us room to interpolate.
							var j = ((i + 1 - previousFractional) * rate / targetRate) - 1;
							var a = Math.floor(j);
							var b = Math.ceil(j);

							var out;
							if (a == b) {
								out = inputSample(a);
							} else {
								out = inputSample(a) * (b - j) +
								      inputSample(b) * (j - a);
							}

							output[i] = adjustment * out;
						}
					};
				} else {
					// Input rate is higher than the target rate.
					// For now, discard extra samples.
					interpolate = function(input, output, previous, adjustment) {
						for (var i = 0; i < output.length; i++) {
							output[i] = adjustment * input[(i * input.length / output.length) | 0];
						}
					};
				}

				var adjustment = 1;
				if (targetChannels > channels) {
					// assume mono -> stereo conversion
					// tone down the loudness rather than duplicating to both channels
					adjustment = Math.SQRT1_2;
				}

				for (var channel = 0; channel < targetChannels; channel++) {
					var inputChannel = channel;
					if (channel >= channels) {
						// Flash forces output to stereo; if input is mono, dupe the first channel
						inputChannel = 0;
					}
					var input = sampleData[inputChannel],
						output = new Float32Array(outputSamples),
						previous = this._resampleLastSampleData ? this._resampleLastSampleData[inputChannel] : undefined;

					interpolate(input, output, previous, adjustment);

					newSamples.push(output);
				}
				this._resampleFractional = remainingFractional;
				this._resampleLastSampleData = sampleData;
				return newSamples;
			}
		};


		/**
		 * Queue up some audio data for playback.
		 *
		 * @param {SampleBuffer} sampleData - input data to queue up for playback
		 *
		 * @todo throw if data invalid or uneven
		 */
		AudioFeeder.prototype.bufferData = function(sampleData) {
			if (this._backend) {
				var samples = this._resample(sampleData);

				// NB! it is important all samples go through tempoChanger
				//  it is built to switch to just copying if tempo = 1.0
				//  but it still needs the samples to transition smoothly
				//  and keep an accurate time map
				samples = this._tempoChanger.process(samples);

				this._backend.appendBuffer(samples);
			} else {
				throw 'Invalid state: AudioFeeder cannot bufferData before init';
			}
		};

		/**
		 * Get an object with information about the current playback state.
		 *
		 * @return {PlaybackState} - info about current playback state
		 */
		AudioFeeder.prototype.getPlaybackState = function() {
			if (this._backend) {
				var state = this._backend.getPlaybackState();
				state.outputPlaybackPosition = state.playbackPosition; // as _backend works only in output time
				state.playbackPosition = this._tempoChanger.mapOutputToInputTime(state.outputPlaybackPosition);
				return state;
			} else {
				throw 'Invalid state: AudioFeeder cannot getPlaybackState before init';
			}
		};

		/**
		 * Checks if audio system is ready and calls the callback when ready
		 * to begin playback.
		 *
		 * This will wait for the Flash shim to load on IE 10/11; waiting
		 * is not required when using native Web Audio but you should use
		 * this callback to support older browsers.
		 *
		 * @param {function} callback - called when ready
		 */
		AudioFeeder.prototype.waitUntilReady = function(callback) {
			if (this._backend) {
				this._backend.waitUntilReady(callback);
			} else {
				throw 'Invalid state: AudioFeeder cannot waitUntilReady before init';
			}
		};

		/**
		 * Start/continue playback as soon as possible.
		 *
		 * You should buffer some audio ahead of time to avoid immediately
		 * running into starvation.
		 */
		AudioFeeder.prototype.start = function() {
			if (this._backend) {
				this._backend.start();
			} else {
				throw 'Invalid state: AudioFeeder cannot start before init';
			}
		};

		/**
		 * Stop/pause playback as soon as possible.
		 *
		 * Audio that has been buffered but not yet sent to the device will
		 * remain buffered, and can be continued with another call to start().
		 */
		AudioFeeder.prototype.stop = function() {
			if (this._backend) {
				this._backend.stop();
			} else {
				throw 'Invalid state: AudioFeeder cannot stop before init';
			}
		};

		/**
		 * Flush any queued data out of the system.
		 */
		AudioFeeder.prototype.flush = function() {
			this._resampleFractional = 0;
			this._resampleLastSampleData = undefined;
			if (this._backend) {
				this._tempoChanger.flush(this.durationBuffered);
				this._backend.flush();
			} else {
				throw 'Invalid state: AudioFeeder cannot flush before init';
			}
		}

		/**
		 * Close out the audio channel. The AudioFeeder instance will no
		 * longer be usable after closing.
		 *
		 * @todo close out the AudioContext if no longer needed
		 * @todo make the instance respond more consistently once closed
		 */
		AudioFeeder.prototype.close = function() {
			if (this._backend) {
				this._backend.close();
				this._backend = null;
			}
		};

		/**
		 * Synchronous callback when we find we're out of buffered data.
		 *
		 * @type {function}
		 */
		AudioFeeder.prototype.onstarved = null;

		/**
		 * Asynchronous callback when we're running low on buffered data.
		 *
		 * @type {function}
		 */
		AudioFeeder.prototype.onbufferlow = null;

		/**
		 * Is the AudioFeeder class supported in this browser?
		 *
		 * Note that it's still possible to be supported but not work, for instance
		 * if there are no audio output devices but the APIs are available.
		 *
		 * @returns {boolean} - true if Web Audio API is available
		 */
		AudioFeeder.isSupported = function() {
			return !!Float32Array && (WebAudioBackend.isSupported() || FlashBackend.isSupported());
		};

		/**
		 * Force initialization of the default Web Audio API context, if applicable.
		 *
		 * Some browsers (such as mobile Safari) disable audio output unless
		 * first triggered from a UI event handler; call this method as a hint
		 * that you will be starting up an AudioFeeder soon but won't have data
		 * for it until a later callback.
		 *
		 * @returns {AudioContext|null} - initialized AudioContext instance, if applicable
		 */
		AudioFeeder.initSharedAudioContext = function() {
			if (WebAudioBackend.isSupported()) {
				return WebAudioBackend.initSharedAudioContext();
			} else {
				return null;
			}
		};

		module.exports = AudioFeeder;

	})();


/***/ }),
/* 1 */
/***/ (function(module, exports) {

	/**
	 * @file Abstraction around a queue of audio buffers.
	 *
	 * @author Brion Vibber <brion@pobox.com>
	 * @copyright (c) 2013-2016 Brion Vibber
	 * @license MIT
	 */

	/**
	 * Constructor for BufferQueue class.
	 * @class
	 * @param {number} numChannels - channel count to validate against
	 * @param {number} bufferSize - desired size of pre-chunked output buffers, in samples
	 *
	 * @classdesc
	 * Abstraction around a queue of audio buffers.
	 *
	 * Stuff input buffers of any length in via {@link BufferQueue#appendBuffer},
	 * check how much is queued with {@link BufferQueue#sampleCount}, and pull out
	 * data in fixed-size chunks from the start with {@link BufferQueue#shift}.
	 */
	function BufferQueue(numChannels, bufferSize) {
	  if (numChannels < 1 || numChannels !== Math.round(numChannels)) {
	    throw 'Invalid channel count for BufferQueue';
	  }
	  this.channels = numChannels;
	  this.bufferSize = bufferSize;
	  this.flush();
	}

	/**
	 * Flush any data out of the queue, resetting to empty state.
	 */
	BufferQueue.prototype.flush = function() {
	  this._buffers = [];
	  this._pendingBuffer = this.createBuffer(this.bufferSize);
	  this._pendingPos = 0;
	};

	/**
	 * Count how many samples are queued up
	 *
	 * @returns {number} - total count in samples
	 */
	BufferQueue.prototype.sampleCount = function() {
	  var count = 0;
	  this._buffers.forEach(function(buffer) {
	    count += buffer[0].length;
	  });
	  return count;
	};

	/**
	 * Create an empty audio sample buffer with space for the given count of samples.
	 *
	 * @param {number} sampleCount - number of samples to reserve in the buffer
	 * @returns {SampleBuffer} - empty buffer
	 */
	BufferQueue.prototype.createBuffer = function(sampleCount) {
	  var output = [];
	  for (var i = 0; i < this.channels; i++) {
	    output[i] = new Float32Array(sampleCount);
	  }
	  return output;
	};

	/**
	 * Validate a buffer for correct object layout
	 *
	 * @param {SampleBuffer} buffer - an audio buffer to check
	 * @returns {boolean} - true if input buffer is valid
	 */
	BufferQueue.prototype.validate = function(buffer) {
	  if (buffer.length !== this.channels) {
	    return false;
	  }

	  var sampleCount;
	  for (var i = 0; i < buffer.length; i++) {
	    var channelData = buffer[i];
	    if (!(channelData instanceof Float32Array)) {
	      return false;
	    }
	    if (i == 0) {
	      sampleCount = channelData.length;
	    } else if (channelData.length !== sampleCount) {
	      return false;
	    }
	  }

	  return true;
	};

	/**
	 * Append a buffer of input data to the queue...
	 *
	 * @param {SampleBuffer} sampleData - an audio buffer to append
	 * @throws exception on invalid input
	 */
	BufferQueue.prototype.appendBuffer = function(sampleData) {
	  if (!this.validate(sampleData)) {
	    throw "Invalid audio buffer passed to BufferQueue.appendBuffer";
	  }

	  var firstChannel = sampleData[0],
	    sampleCount = firstChannel.length;

	  // @todo this still seems kinda inefficient
	  var channels = this.channels;
	  var pendingPos = this._pendingPos;
	  var pendingBuffer = this._pendingBuffer;
	  var bufferSize = this.bufferSize;
	  for (var i = 0; i < sampleCount; i++) {
	    for (var channel = 0; channel < channels; channel++) {
	      pendingBuffer[channel][pendingPos] = sampleData[channel][i];
	    }
	    if (++pendingPos == bufferSize) {
	      this._buffers.push(pendingBuffer);
	      pendingPos = this._pendingPos = 0;
	      pendingBuffer = this._pendingBuffer = this.createBuffer(bufferSize);
	    }
	  }
	  this._pendingPos = pendingPos;
	};

	/**
	 * Unshift the given sample buffer onto the beginning of the buffer queue.
	 *
	 * @param {SampleBuffer} sampleData - an audio buffer to prepend
	 * @throws exception on invalid input
	 *
	 * @todo this is currently pretty inefficient as it rechunks all the buffers.
	 */
	BufferQueue.prototype.prependBuffer = function(sampleData) {
	  if (!this.validate(sampleData)) {
	    throw "Invalid audio buffer passed to BufferQueue.prependBuffer";
	  }

	  // Since everything is pre-chunked in the queue, we're going to have
	  // to pull everything out and re-append it.
	  var buffers = this._buffers.slice(0)
	  buffers.push(this.trimBuffer(this._pendingBuffer, 0, this._pendingPos));

	  this.flush();
	  this.appendBuffer(sampleData);

	  // Now put back any old buffers, dividing them up into chunks.
	  for (var i = 0; i < buffers.length; i++) {
	    this.appendBuffer(buffers[i]);
	  }
	};

	/**
	 * Shift out a buffer from the head of the queue, containing a maximum of
	 * {@link BufferQueue#bufferSize} samples; if there are not enough samples
	 * you may get a shorter buffer. Call {@link BufferQueue#sampleCount} to
	 * check if enough samples are available for your needs.
	 *
	 * @returns {SampleBuffer} - an audio buffer with zero or more samples
	 */
	BufferQueue.prototype.nextBuffer = function() {
	  if (this._buffers.length) {
	    return this._buffers.shift();
	  } else {
	    var trimmed = this.trimBuffer(this._pendingBuffer, 0, this._pendingPos);
	    this._pendingBuffer = this.createBuffer(this.bufferSize);
	    this._pendingPos = 0;
	    return trimmed;
	  }
	};

	/**
	 * Trim a buffer down to a given maximum sample count.
	 * Any additional samples will simply be cropped off of the view.
	 * If no trimming is required, the same buffer will be returned.
	 *
	 * @param {SampleBuffer} sampleData - input data
	 * @param {number} start - sample number to start at
	 * @param {number} maxSamples - count of samples to crop to
	 * @returns {SampleBuffer} - output data with at most maxSamples samples
	 */
	BufferQueue.prototype.trimBuffer = function(sampleData, start, maxSamples) {
	  var bufferLength = sampleData[0].length,
	    end = start + Math.min(maxSamples, bufferLength);
	  if (start == 0 && end >= bufferLength) {
	    return sampleData;
	  } else {
	    var output = [];
	    for (var i = 0; i < this.channels; i++) {
	      output[i] = sampleData[i].subarray(start, end);
	    }
	    return output;
	  }
	};

	module.exports = BufferQueue;


/***/ }),
/* 2 */
/***/ (function(module, exports, __webpack_require__) {

	/**
	 * @file Web Audio API backend for AudioFeeder
	 * @author Brion Vibber <brion@pobox.com>
	 * @copyright (c) 2013-2016 Brion Vibber
	 * @license MIT
	 */

	(function() {

	  var AudioContext = window.AudioContext || window.webkitAudioContext,
	    BufferQueue = __webpack_require__(1),
	    nextTick = __webpack_require__(3);

	  /**
	   * Constructor for AudioFeeder's Web Audio API backend.
	   * @class
	   * @param {number} numChannels - requested count of output channels
	   * @param {number} sampleRate - requested sample rate for output
	   * @param {Object} options - pass URL path to directory containing 'dynamicaudio.swf' in 'base' parameter
	   *
	   * @classdesc Web Audio API output backend for AudioFeeder.
	   * Maintains an internal {@link BufferQueue} of audio samples to be output on demand.
	   */
	  function WebAudioBackend(numChannels, sampleRate, options) {
	    var context = options.audioContext || WebAudioBackend.initSharedAudioContext();

	    this._context = context;


	    /*
	     * Optional audio node can be provided to connect the feeder to
	     * @type {AudioNode}
	     */
	    this.output = options.output || context.destination

	    /**
	     * Actual sample rate supported for output, in Hz
	     * @type {number}
	     * @readonly
	     */
	    this.rate = context.sampleRate;

	    /**
	     * Actual count of channels supported for output
	     * @type {number}
	     * @readonly
	     *
	     * Note currently this is forced to 2. More channels (5.1, etc)
	     * aren't well specified and need to be tested. Fewer channels
	     * (mono) ends up too loud unless we reduce amplitude in the
	     * resampling operation.
	     */
	    //this.channels = Math.min(numChannels, 2); // @fixme remove this limit
	    this.channels = 2; // @fixme remove this limit

	    if (options.bufferSize) {
	        this.bufferSize = (options.bufferSize | 0);
	    }
	    this.bufferThreshold = 2 * this.bufferSize;

	    this._bufferQueue = new BufferQueue(this.channels, this.bufferSize);
	    this._playbackTimeAtBufferTail = context.currentTime;
	    this._queuedTime = 0;
	    this._delayedTime = 0;
	    this._dropped = 0;
	    this._liveBuffer = this._bufferQueue.createBuffer(this.bufferSize);

	    // @todo support new audio worker mode too
	    if (context.createScriptProcessor) {
	      this._node = context.createScriptProcessor(this.bufferSize, 0, this.channels);
	    } else if (context.createJavaScriptNode) {
	      // In older Safari versions
	      this._node = context.createJavaScriptNode(this.bufferSize, 0, this.channels);
	    } else {
	      throw new Error("Bad version of web audio API?");
	    }
	  }

	  /**
	   * Size of output buffers in samples, as a hint for latency/scheduling
	   * @type {number}
	   * @readonly
	   */
	  WebAudioBackend.prototype.bufferSize = 4096;

	  /**
	   * Remaining sample count at which a 'bufferlow' event will be triggered.
	   *
	   * Will be pinged when falling below bufferThreshold or bufferSize,
	   * whichever is larger.
	   *
	   * @type {number}
	   */
	  WebAudioBackend.prototype.bufferThreshold = 8192;

	  /**
	   * Internal volume property backing.
	   * @type {number}
	   * @access private
	   */
	  WebAudioBackend.prototype._volume = 1;

	  /**
		 * Volume multiplier, defaults to 1.0.
		 * @name volume
		 * @type {number}
		 */
		Object.defineProperty(WebAudioBackend.prototype, 'volume', {
			get: function getVolume() {
	      return this._volume;
			},
			set: function setVolume(val) {
	      this._volume = +val;
			}
		});

	  /**
	   * Internal muted property backing.
	   * @type {number}
	   * @access private
	   */
	  WebAudioBackend.prototype._muted = false;

	  /**
		 * Is the backend currently set to mute output?
		 * When muted, this overrides the volume property.
		 *
		 * @type {boolean}
		 */
		Object.defineProperty(WebAudioBackend.prototype, 'muted', {
	 		get: function getMuted() {
	      return this._muted;
	 		},
	 		set: function setMuted(val) {
	      this._muted = !!val;
	 		}
	 	});

	  /**
	   * onaudioprocess event handler for the ScriptProcessorNode
	   * @param {AudioProcessingEvent} event - audio processing event object
	   * @access private
	   */
	  WebAudioBackend.prototype._audioProcess = function(event) {
	    var channel, input, output, i, playbackTime;
	    if (typeof event.playbackTime === 'number') {
	      playbackTime = event.playbackTime;
	    } else {
	      // Safari 6.1 hack
	      playbackTime = this._context.currentTime + (this.bufferSize / this.rate);
	    }

	    var expectedTime = this._playbackTimeAtBufferTail;
	    if (expectedTime < playbackTime) {
	      // we may have lost some time while something ran too slow
	      this._delayedTime += (playbackTime - expectedTime);
	    }

	    if (this._bufferQueue.sampleCount() < this.bufferSize) {
	      // We might be in a throttled background tab; go ping the decoder
	      // and let it know we need more data now!
	      // @todo use standard event firing?
	      if (this.onstarved) {
	        this.onstarved();
	      }
	    }

	    // If we still haven't got enough data, write a buffer of silence
	    // to all channels and record an underrun event.
	    // @todo go ahead and output the data we _do_ have?
	    if (this._bufferQueue.sampleCount() < this.bufferSize) {
	      for (channel = 0; channel < this.channels; channel++) {
	        output = event.outputBuffer.getChannelData(channel);
	        for (i = 0; i < this.bufferSize; i++) {
	          output[i] = 0;
	        }
	      }
	      this._dropped++;
	      return;
	    }

	    var volume = (this.muted ? 0 : this.volume);

	    // Actually get that data and write it out...
	    var inputBuffer = this._bufferQueue.nextBuffer();
	    if (inputBuffer[0].length < this.bufferSize) {
	      // This should not happen, but trust no invariants!
	      throw 'Audio buffer not expected length.';
	    }
	    for (channel = 0; channel < this.channels; channel++) {
	      input = inputBuffer[channel];

	      // Save this buffer data for later in case we pause
	      this._liveBuffer[channel].set(inputBuffer[channel]);

	      // And play it out with volume applied...
	      output = event.outputBuffer.getChannelData(channel);
	      for (i = 0; i < input.length; i++) {
	        output[i] = input[i] * volume;
	      }
	    }
	    this._queuedTime += (this.bufferSize / this.rate);
	    this._playbackTimeAtBufferTail = playbackTime + (this.bufferSize / this.rate);

	    if (this._bufferQueue.sampleCount() < Math.max(this.bufferSize, this.bufferThreshold)) {
	      // Let the decoder know ahead of time we're running low on data.
	      // @todo use standard event firing?
	      if (this.onbufferlow) {
	        nextTick(this.onbufferlow.bind(this));
	      }
	    }
	  };


	  /**
	   * Return a count of samples that have been queued or output but not yet played.
	   *
	   * @returns {number} - sample count
	   * @access private
	   */
	  WebAudioBackend.prototype._samplesQueued = function() {
	    var bufferedSamples = this._bufferQueue.sampleCount();
	    var remainingSamples = Math.floor(this._timeAwaitingPlayback() * this.rate);

	    return bufferedSamples + remainingSamples;
	  };

	  /**
	   * Return time duration between the present and the endpoint of audio
	   * we have already sent out from our queue to Web Audio.
	   *
	   * @returns {number} - seconds
	   */
	  WebAudioBackend.prototype._timeAwaitingPlayback = function() {
	    return Math.max(0, this._playbackTimeAtBufferTail - this._context.currentTime);
	  };

	  /**
	   * Get info about current playback state.
	   *
	   * @return {PlaybackState} - info about current playback state
	   */
	  WebAudioBackend.prototype.getPlaybackState = function() {
	    return {
	      playbackPosition: this._queuedTime - this._timeAwaitingPlayback(),
	      samplesQueued: this._samplesQueued(),
	      dropped: this._dropped,
	      delayed: this._delayedTime
	    };
	  };

	  /**
	   * Wait asynchronously until the backend is ready before continuing.
	   *
	   * This will always call immediately for the Web Audio API backend,
	   * as there is no async setup process.
	   *
	   * @param {function} callback - to be called when ready
	   */
	  WebAudioBackend.prototype.waitUntilReady = function(callback) {
	    callback();
	  };

	  /**
	   * Append a buffer of audio data to the output queue for playback.
	   *
	   * Audio data must be at the expected sample rate; resampling is done
	   * upstream in {@link AudioFeeder}.
	   *
	   * @param {SampleBuffer} sampleData - audio data at target sample rate
	   */
	  WebAudioBackend.prototype.appendBuffer = function(sampleData) {
	    this._bufferQueue.appendBuffer(sampleData);
	  };

	  /**
	   * Start playback.
	   *
	   * Audio should have already been queued at this point,
	   * or starvation may occur immediately.
	   */
	  WebAudioBackend.prototype.start = function() {
	    this._node.onaudioprocess = this._audioProcess.bind(this);
	    this._node.connect(this.output);
	    this._playbackTimeAtBufferTail = this._context.currentTime;
	  };

	  /**
	   * Stop playback, but don't release resources or clear the buffers.
	   * We'll probably come back soon.
	   */
	  WebAudioBackend.prototype.stop = function() {
	    if (this._node) {
	      var timeRemaining = this._timeAwaitingPlayback();
	      if (timeRemaining > 0) {
	        // We have some leftover samples that got queued but didn't get played.
	        // Unshift them back onto the beginning of the buffer.
	        // @todo make this not a horrible hack
	        var samplesRemaining = Math.round(timeRemaining * this.rate),
	            samplesAvailable = this._liveBuffer ? this._liveBuffer[0].length : 0;
	        if (samplesRemaining > samplesAvailable) {
	          //console.log('liveBuffer size ' + samplesRemaining + ' vs ' + samplesAvailable);
	          this._bufferQueue.prependBuffer(this._liveBuffer);
	          this._bufferQueue.prependBuffer(
	            this._bufferQueue.createBuffer(samplesRemaining - samplesAvailable));
	        } else {
	          this._bufferQueue.prependBuffer(
	            this._bufferQueue.trimBuffer(this._liveBuffer, samplesAvailable - samplesRemaining, samplesRemaining));
	        }
	        this._playbackTimeAtBufferTail -= timeRemaining;
	      }
	      this._node.onaudioprocess = null;
	      this._node.disconnect();
	    }
	  };

	  /**
	   * Flush any queued data out of the system.
	   */
	  WebAudioBackend.prototype.flush = function() {
	    this._bufferQueue.flush();
	  };

	  /**
	   * Close out the playback system and release resources.
	   *
	   * @todo consider releasing the AudioContext when possible
	   */
	  WebAudioBackend.prototype.close = function() {
	    this.stop();

	    this._context = null;
	  };

	  /**
	   * Synchronous callback for when we run out of input data
	   *
	   * @type function|null
	   */
	  WebAudioBackend.prototype.onstarved = null;

	  /**
	   * Asynchronous callback for when the buffer runs low and
	   * should be refilled soon.
	   *
	   * @type function|null
	   */
	  WebAudioBackend.prototype.onbufferlow = null;

	  /**
	   * Check if Web Audio API appears to be supported.
	   *
	   * Note this is somewhat optimistic; will return true even if there are no
	   * audio devices available, as long as the API is present.
	   *
	   * @returns {boolean} - true if this browser appears to support Web Audio API
	   */
	  WebAudioBackend.isSupported = function() {
	    return !!AudioContext;
	  };

	  /**
	   * Holder of audio context to be used/reused by WebAudioBackend.
	   * @see {WebAudioBackend#initSharedAudioContext}
	   *
	   * @type {AudioContext}
	   */
	  WebAudioBackend.sharedAudioContext = null;

	  /**
		 * Force initialization of the default Web Audio API context.
		 *
		 * Some browsers (such as mobile Safari) disable audio output unless
		 * first triggered from a UI event handler; call this method as a hint
		 * that you will be starting up an AudioFeeder soon but won't have data
		 * for it until a later callback.
	   *
	   * @returns {AudioContext|null} - initialized AudioContext instance, if applicable
		 */
	  WebAudioBackend.initSharedAudioContext = function() {
			if (!WebAudioBackend.sharedAudioContext) {
				if (WebAudioBackend.isSupported()) {
					// We're only allowed 4 contexts on many browsers
					// and there's no way to discard them (!)...
					var context = new AudioContext(),
						node;
					if (context.createScriptProcessor) {
						node = context.createScriptProcessor(1024, 0, 2);
					} else if (context.createJavaScriptNode) {
						node = context.createJavaScriptNode(1024, 0, 2);
					} else {
						throw new Error( "Bad version of web audio API?" );
					}

					// Don't actually run any audio, just start & stop the node
					node.connect(context.destination);
					node.disconnect();

	        // So far so good. Keep it around!
	        WebAudioBackend.sharedAudioContext = context;
				}
			}
	    return WebAudioBackend.sharedAudioContext;
		};

	  module.exports = WebAudioBackend;

	})();


/***/ }),
/* 3 */
/***/ (function(module, exports) {

	module.exports = (function() {
		// Don't try to check for setImmediate directly; webpack implements
		// it using setTimeout which will be throttled in background tabs.
		// Checking directly on the global window object skips this interference.
		if (typeof window.setImmediate !== 'undefined') {
			return window.setImmediate;
		}

		// window.postMessage goes straight to the event loop, no throttling.
		if (window && window.postMessage) {
			var nextTickQueue = [];
			window.addEventListener('message', function(event) {
				if (event.source === window) {
					var data = event.data;
					if (typeof data === 'object' && data.nextTickBrowserPingMessage) {
						var callback = nextTickQueue.pop();
						if (callback) {
							callback();
						}
					}
				}
			});
			return function(callback) {
				nextTickQueue.push(callback);
				window.postMessage({
					nextTickBrowserPingMessage: true
				}, document.location.toString())
			};
		}

		// Timeout fallback may be poor in background tabs
		return function(callback) {
			setTimeout(callback, 0);
		}
	})();


/***/ }),
/* 4 */
/***/ (function(module, exports, __webpack_require__) {

	(function() {

	  /* global ActiveXObject */
	  var dynamicaudio_swf = __webpack_require__(5);

	  var nextTick = __webpack_require__(3);

	  /**
	   * Constructor for AudioFeeder's Flash audio backend.
	   * @class
	   * @param {number} numChannels - requested count of output channels (actual will be fixed at 2)
	   * @param {number} sampleRate - requested sample rate for output (actual will be fixed at 44.1 kHz)
	   * @param {Object} options - pass URL path to directory containing 'dynamicaudio.swf' in 'base' parameter
	   *
	   * @classdesc Flash audio output backend for AudioFeeder.
	   * Maintains a local queue of data to be sent down to the Flash shim.
	   * Resampling to stereo 44.1 kHz is done upstream in AudioFeeder.
	   */
	  var FlashBackend = function(numChannels, sampleRate, options) {
	    options = options || {};
	    var flashOptions = {};
	    if (typeof options.base === 'string') {
	      // @fixme replace the version string with an auto-updateable one
	      flashOptions.swf = options.base + '/' + dynamicaudio_swf;
	    }
	    if (options.bufferSize) {
	      this.bufferSize = (options.bufferSize | 0);
	    }

	    this._flashaudio = new DynamicAudio(flashOptions);
	    this._flashBuffer = '';
	    this._cachedFlashState = null;
	    this._cachedFlashTime = 0;
	    this._cachedFlashInterval = 185; // resync state no more often than every X ms

	    this._waitUntilReadyQueue = [];
	    this.onready = function() {
	        this._flashaudio.flashElement.setBufferSize(this.bufferSize);
	        this._flashaudio.flashElement.setBufferThreshold(this.bufferThreshold);
	        while (this._waitUntilReadyQueue.length) {
	            var callback = this._waitUntilReadyQueue.shift();
	            callback.apply(this);
	        }
	    };
	    this.onlog = function(msg) {
	        console.log('AudioFeeder FlashBackend: ' + msg);
	    }

	    this.bufferThreshold = this.bufferSize * 2;

	    var events = {
	        'ready': 'sync',
	        'log': 'sync',
	        'starved': 'sync',
	        'bufferlow': 'async'
	    };
	    this._callbackName = 'AudioFeederFlashBackendCallback' + this._flashaudio.id;
	    var self = this;
	    window[this._callbackName] = (function(eventName) {
	        var method = events[eventName],
	            callback = this['on' + eventName];
	        if (method && callback) {
	            if (method === 'async') {
	                nextTick(callback.bind(this));
	            } else {
	                callback.apply(this, Array.prototype.slice.call(arguments, 1));
	                this._flushFlashBuffer();
	            }
	        }
	    }).bind(this);
	  };

	  /**
	   * Actual sample rate supported for output, in Hz
	   * Fixed to 44.1 kHz for Flash backend.
	   * @type {number}
	   * @readonly
	   */
	  FlashBackend.prototype.rate = 44100;

	  /**
	   * Actual count of channels supported for output
	   * Fixed to stereo for Flash backend.
	   * @type {number}
	   * @readonly
	   */
	  FlashBackend.prototype.channels = 2;

	  /**
	   * Buffer size hint.
	   * @type {number}
	   * @readonly
	   */
	  FlashBackend.prototype.bufferSize = 4096;

	  /**
	   * Internal bufferThreshold property backing.
	   * @type {number}
	   * @access private
	   */
	  FlashBackend.prototype._bufferThreshold = 8192;

	  /**
	   * Remaining sample count at which a 'bufferlow' event will be triggered.
	   *
	   * Will be pinged when falling below bufferThreshold or bufferSize,
	   * whichever is larger.
	   *
	   * @type {number}
	   */
	  Object.defineProperty(FlashBackend.prototype, 'bufferThreshold', {
	    get: function getBufferThreshold() {
	      return this._bufferThreshold;
	    },
	    set: function setBufferThreshold(val) {
	      this._bufferThreshold = val | 0;
	      this.waitUntilReady((function() {
	        this._flashaudio.flashElement.setBufferThreshold(this._bufferThreshold);
	      }).bind(this));
	    }
	  });

	  /**
	   * Internal volume property backing.
	   * @type {number}
	   * @access private
	   */
	  FlashBackend.prototype._volume = 1;

	  /**
		 * Volume multiplier, defaults to 1.0.
		 * @name volume
		 * @type {number}
		 */
		Object.defineProperty(FlashBackend.prototype, 'volume', {
			get: function getVolume() {
	      return this._volume;
			},
			set: function setVolume(val) {
	      this._volume = +val;
	      this.waitUntilReady(this._flashVolumeUpdate.bind(this));
			}
		});

	  /**
	   * Internal muted property backing.
	   * @type {number}
	   * @access private
	   */
	  FlashBackend.prototype._muted = false;

	  /**
		 * Is the backend currently set to mute output?
		 * When muted, this overrides the volume property.
		 *
		 * @type {boolean}
		 */
		Object.defineProperty(FlashBackend.prototype, 'muted', {
	 		get: function getMuted() {
	      return this._muted;
	 		},
	 		set: function setMuted(val) {
	      this._muted = !!val;
	      this.waitUntilReady(this._flashVolumeUpdate.bind(this));
	 		}
	 	});

	  /**
	   * Are we paused/idle?
	   * @type {boolean}
	   * @access private
	   */
	  FlashBackend.prototype._paused = true;

	  /**
	   * Pass the currently configured muted+volume state down to the Flash plugin
	   * @access private
	   */
	  FlashBackend.prototype._flashVolumeUpdate = function() {
	    if (this._flashaudio && this._flashaudio.flashElement && this._flashaudio.flashElement.setVolume) {
	      this._flashaudio.flashElement.setVolume(this.muted ? 0 : this.volume);
	    }
	  }

	  /**
	   * Reordering of output for the Flash fallback.
	   * Input data must be pre-resampled to the correct sample rate.
	   * Mono input is doubled to stereo; more than 2 channels are dropped.
	   *
	   * @param {SampleBuffer} samples - input data as separate channels of 32-bit float
	   * @returns {Float32Array} - interleaved stereo 32-bit float output
	   * @access private
	   *
	   * @todo handle input with higher channel counts better
	   */
	  FlashBackend.prototype._resampleFlash = function(samples) {
	  	var samplecount = samples[0].length;
	  	var newSamples = new Float32Array(samplecount * 2);
	  	var chanLeft = samples[0];
	  	var chanRight = this.channels > 1 ? samples[1] : chanLeft;
	  	for(var s = 0; s < samplecount; s++) {
	  		var idx = s;
	  		var idx_out = s * 2;

	  		newSamples[idx_out] = chanLeft[idx];
	  		newSamples[idx_out + 1] = chanRight[idx];
	  	}
	  	return newSamples;
	  };

	  var binBytes = [];
	  for (var i = 0; i < 256; i++) {
	    binBytes[i] = String.fromCharCode(i + 0xe000);
	  }
	  function binaryString(buffer) {
	    var samples = new Uint8Array(buffer);
	    var len = samples.length;
	    var str = '';
	    for (var i = 0; i < len; i += 8) {
	      str += binBytes[samples[i]];
	      str += binBytes[samples[i + 1]];
	      str += binBytes[samples[i + 2]];
	      str += binBytes[samples[i + 3]];
	      str += binBytes[samples[i + 4]];
	      str += binBytes[samples[i + 5]];
	      str += binBytes[samples[i + 6]];
	      str += binBytes[samples[i + 7]];
	    }
	    return str;
	  }

	  /**
	   * Send any pending data off to the Flash plugin.
	   *
	   * @access private
	   */
	  FlashBackend.prototype._flushFlashBuffer = function() {
	    var chunk = this._flashBuffer,
	      flashElement = this._flashaudio.flashElement;

	    this._flashBuffer = '';

	    if (chunk.length > 0) {
	      this._cachedFlashState = flashElement.write(chunk);
	      this._cachedFlashTime = Date.now();
	    }
	  };

	  /**
	   * Append a buffer of audio data to the output queue for playback.
	   *
	   * Audio data must be at the expected sample rate; resampling is done
	   * upstream in {@link AudioFeeder}.
	   *
	   * @param {SampleBuffer} sampleData - audio data at target sample rate
	   */
	  FlashBackend.prototype.appendBuffer = function(sampleData) {
	    var resamples = this._resampleFlash(sampleData);
	    if (resamples.length > 0) {
	      var str = binaryString(resamples.buffer);
	      this._flashBuffer += str;
	      if (this._flashBuffer.length >= this.bufferSize * 8) {
	        // consolidate multiple consecutive tiny buffers in one pass;
	        // pushing data to Flash is relatively expensive on slow machines
	        this._flushFlashBuffer();
	      }
	    }
	  };

	  /**
	   * Get info about current playback state.
	   *
	   * @return {PlaybackState} - info about current playback state
	   */
	  FlashBackend.prototype.getPlaybackState = function() {
	    if (this._flashaudio && this._flashaudio.flashElement && this._flashaudio.flashElement.write) {
	      var now = Date.now(),
	        delta = this._paused ? 0 : (now - this._cachedFlashTime),
	        state;
	      if (this._cachedFlashState && delta < this._cachedFlashInterval) {
	        var cachedFlashState = this._cachedFlashState;
	        state = {
	          playbackPosition: cachedFlashState.playbackPosition + delta / 1000,
	          samplesQueued: cachedFlashState.samplesQueued -
	            Math.max(0, Math.round(delta * this.rate / 1000)),
	          dropped: cachedFlashState.dropped,
	          delayed: cachedFlashState.delayed
	        };
	      } else {
	        state = this._flashaudio.flashElement.getPlaybackState();
	        this._cachedFlashState = state;
	        this._cachedFlashTime = now;
	      }
	      state.samplesQueued += this._flashBuffer.length / 8;
	      return state;
	    } else {
	      //console.log('getPlaybackState USED TOO EARLY');
	      return {
	        playbackPosition: 0,
	        samplesQueued: 0,
	        dropped: 0,
	        delayed: 0
	      };
	    }
	  };

	  /**
	   * Wait until the backend is ready to start, then call the callback.
	   *
	   * @param {function} callback - called on completion
	   * @todo handle fail case better?
	   */
	  FlashBackend.prototype.waitUntilReady = function(callback) {
	    if (this._flashaudio && this._flashaudio.flashElement.write) {
	      callback.apply(this);
	    } else {
	      this._waitUntilReadyQueue.push(callback);
	    }
	  };

	  /**
	   * Start playback.
	   *
	   * Audio should have already been queued at this point,
	   * or starvation may occur immediately.
	   */
	  FlashBackend.prototype.start = function() {
	    this._flushFlashBuffer();
	    this._flashaudio.flashElement.start();
	    this._paused = false;
	    this._cachedFlashState = null;
	  };

	  /**
	   * Stop playback, but don't release resources or clear the buffers.
	   * We'll probably come back soon.
	   */
	  FlashBackend.prototype.stop = function() {
	    this._flashaudio.flashElement.stop();
	    this._paused = true;
	    this._cachedFlashState = null;
	  };

	  /**
	   * Flush any queued data out of the system.
	   */
	  FlashBackend.prototype.flush = function() {
	    this._flashBuffer = '';
	    this._flashaudio.flashElement.flush();
	    this._cachedFlashState = null;
	  };

	  /**
	   * Close out the playback system and release resources.
	   */
	  FlashBackend.prototype.close = function() {
	    this.stop();

	    var wrapper = this._flashaudio.flashWrapper;
	    wrapper.parentNode.removeChild(wrapper);
	    this._flashaudio = null;
	    delete window[this._callbackName];
	  };

	  /**
	   * Synchronous callback for when we run out of input data
	   *
	   * @type function|null
	   */
	  FlashBackend.prototype.onstarved = null;

	  /**
	   * Asynchronous callback for when the buffer runs low and
	   * should be refilled soon.
	   *
	   * @type function|null
	   */
	  FlashBackend.prototype.onbufferlow = null;

	  /**
	   * Check if the browser appears to support Flash.
	   *
	   * Note this is somewhat optimistic, in that Flash may be supported
	   * but the dynamicaudio.swf file might not load, or it might load
	   * but there might be no audio devices, etc.
	   *
	   * Currently only checks for the ActiveX Flash plugin for Internet Explorer,
	   * as other target browsers support Web Audio API.
	   *
	   * @returns {boolean} - true if this browser appears to support Flash
	   */
	  FlashBackend.isSupported = function() {
			if (navigator.userAgent.indexOf('Trident') !== -1) {
				// We only do the ActiveX test because we only need Flash in
				// Internet Explorer 10/11. Other browsers use Web Audio directly
				// (Edge, Safari) or native playback, so there's no need to test
				// other ways of loading Flash.
				try {
					var obj = new ActiveXObject('ShockwaveFlash.ShockwaveFlash');
					return true;
				} catch(e) {
					return false;
				}
			}
			return false;
	  };

		/** Flash fallback **/

		/*
		The Flash fallback is based on https://github.com/an146/dynamicaudio.js

		This is the contents of the LICENSE file:

		Copyright (c) 2010, Ben Firshman
		All rights reserved.

		Redistribution and use in source and binary forms, with or without
		modification, are permitted provided that the following conditions are met:

		 * Redistributions of source code must retain the above copyright notice, this
		   list of conditions and the following disclaimer.
		 * Redistributions in binary form must reproduce the above copyright notice,
		   this list of conditions and the following disclaimer in the documentation
		   and/or other materials provided with the distribution.
		 * The names of its contributors may not be used to endorse or promote products
		   derived from this software without specific prior written permission.

		THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
		ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
		WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
		DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
		ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
		(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
		LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
		ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
		(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
		SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
		*/


	  /**
	   * Wrapper class for instantiating Flash plugin.
	   *
	   * @constructor
	   * @param {Object} opt - pass 'swf' to override default dynamicaudio.swf URL
	   * @access private
	   */
		function DynamicAudio(opt) {
			this.init(opt);
		}


		DynamicAudio.nextId = 1;

		DynamicAudio.prototype = {
			nextId: null,
			swf: dynamicaudio_swf,

			flashWrapper: null,
			flashElement: null,

			init: function(opts) {
				var self = this;
				self.id = DynamicAudio.nextId++;

				if (opts && typeof opts.swf !== 'undefined') {
					self.swf = opts.swf;
				}


				self.flashWrapper = document.createElement('div');
				self.flashWrapper.id = 'dynamicaudio-flashwrapper-'+self.id;
				// Credit to SoundManager2 for this:
				var s = self.flashWrapper.style;
				s.position = 'fixed';
				s.width = '11px'; // must be at least 6px for flash to run fast
				s.height = '11px';
				s.bottom = s.left = '0px';
				s.overflow = 'hidden';
				self.flashElement = document.createElement('div');
				self.flashElement.id = 'dynamicaudio-flashelement-'+self.id;
				self.flashWrapper.appendChild(self.flashElement);

				document.body.appendChild(self.flashWrapper);

				var id = self.flashElement.id;
	            var params = '<param name="FlashVars" value="objectId=' + self.id + '">';

				self.flashWrapper.innerHTML = "<object id='"+id+"' width='10' height='10' type='application/x-shockwave-flash' data='"+self.swf+"' style='visibility: visible;'><param name='allowscriptaccess' value='always'>" + params + "</object>";
				self.flashElement = document.getElementById(id);
			},
		};

	  module.exports = FlashBackend;

	})();


/***/ }),
/* 5 */
/***/ (function(module, exports, __webpack_require__) {

	module.exports = __webpack_require__.p + "dynamicaudio.swf?version=2c1ce3bfb7e6fa65c26d726a00017a94";

/***/ }),
/* 6 */
/***/ (function(module, exports, __webpack_require__) {

	(function webpackUniversalModuleDefinition(root, factory) {
		if(true)
			module.exports = factory();
		else if(typeof define === 'function' && define.amd)
			define([], factory);
		else if(typeof exports === 'object')
			exports["AudioTempoChanger"] = factory();
		else
			root["AudioTempoChanger"] = factory();
	})(window, function() {
	return /******/ (function(modules) { // webpackBootstrap
	/******/ 	// The module cache
	/******/ 	var installedModules = {};
	/******/
	/******/ 	// The require function
	/******/ 	function __webpack_require__(moduleId) {
	/******/
	/******/ 		// Check if module is in cache
	/******/ 		if(installedModules[moduleId]) {
	/******/ 			return installedModules[moduleId].exports;
	/******/ 		}
	/******/ 		// Create a new module (and put it into the cache)
	/******/ 		var module = installedModules[moduleId] = {
	/******/ 			i: moduleId,
	/******/ 			l: false,
	/******/ 			exports: {}
	/******/ 		};
	/******/
	/******/ 		// Execute the module function
	/******/ 		modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);
	/******/
	/******/ 		// Flag the module as loaded
	/******/ 		module.l = true;
	/******/
	/******/ 		// Return the exports of the module
	/******/ 		return module.exports;
	/******/ 	}
	/******/
	/******/
	/******/ 	// expose the modules object (__webpack_modules__)
	/******/ 	__webpack_require__.m = modules;
	/******/
	/******/ 	// expose the module cache
	/******/ 	__webpack_require__.c = installedModules;
	/******/
	/******/ 	// define getter function for harmony exports
	/******/ 	__webpack_require__.d = function(exports, name, getter) {
	/******/ 		if(!__webpack_require__.o(exports, name)) {
	/******/ 			Object.defineProperty(exports, name, { enumerable: true, get: getter });
	/******/ 		}
	/******/ 	};
	/******/
	/******/ 	// define __esModule on exports
	/******/ 	__webpack_require__.r = function(exports) {
	/******/ 		if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
	/******/ 			Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
	/******/ 		}
	/******/ 		Object.defineProperty(exports, '__esModule', { value: true });
	/******/ 	};
	/******/
	/******/ 	// create a fake namespace object
	/******/ 	// mode & 1: value is a module id, require it
	/******/ 	// mode & 2: merge all properties of value into the ns
	/******/ 	// mode & 4: return value when already ns object
	/******/ 	// mode & 8|1: behave like require
	/******/ 	__webpack_require__.t = function(value, mode) {
	/******/ 		if(mode & 1) value = __webpack_require__(value);
	/******/ 		if(mode & 8) return value;
	/******/ 		if((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;
	/******/ 		var ns = Object.create(null);
	/******/ 		__webpack_require__.r(ns);
	/******/ 		Object.defineProperty(ns, 'default', { enumerable: true, value: value });
	/******/ 		if(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));
	/******/ 		return ns;
	/******/ 	};
	/******/
	/******/ 	// getDefaultExport function for compatibility with non-harmony modules
	/******/ 	__webpack_require__.n = function(module) {
	/******/ 		var getter = module && module.__esModule ?
	/******/ 			function getDefault() { return module['default']; } :
	/******/ 			function getModuleExports() { return module; };
	/******/ 		__webpack_require__.d(getter, 'a', getter);
	/******/ 		return getter;
	/******/ 	};
	/******/
	/******/ 	// Object.prototype.hasOwnProperty.call
	/******/ 	__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };
	/******/
	/******/ 	// __webpack_public_path__
	/******/ 	__webpack_require__.p = "";
	/******/
	/******/
	/******/ 	// Load entry module and return exports
	/******/ 	return __webpack_require__(__webpack_require__.s = 1);
	/******/ })
	/************************************************************************/
	/******/ ([
	/* 0 */
	/***/ (function(module, exports) {


	// Define an allocator and blit function for float arrays
	// Can be used to achieve backwards compatibility down to dark ages pre IE 10 if needed
	// Also reduces code size a little with closure.

	var VH = { 
		float_array: function(len) { return new Float32Array(len); },
		blit: function(src, spos, dest, dpos, len) { dest.set(src.subarray(spos,spos+len),dpos); }
	};

	// Pre-IE10 versions:
	/*VH.prototype.float_array = function(len) { return new Array(len); }
	VH.prototype.blit = function(src, spos, dest, dpos, len) { for(var i=0;i<len;i++) dest[dpos+i] = src[spos+i]; };*/

	module.exports = VH;

	/***/ }),
	/* 1 */
	/***/ (function(module, exports, __webpack_require__) {

	(function() {

		/*
		 * Phase Vocoder for changing tempo of audio without affecting pitch
		 * Originally cross-compiled from HaXe
		 *
		 * Copyright (c) 2015-2019 Margus Niitsoo
		 */

		var VH = __webpack_require__(0);
		var FFT = __webpack_require__(2);

		var AudioTempoChanger = function(opts) {

			/**************************
			* Fill in sensible defaults
			**************************/

			opts = opts || {};
			var sampleRate = opts.sampleRate || 44100;
			var wsizeLog = opts.wsizeLog || 11; // 2048 - good for 44100 
			var chosen_tempo = opts.tempo || 1.0;
			var numChannels = opts.numChannels || 2;

			/**************************
			* Initialize variables
			**************************/

			// Some constants
			var GAIN_DEAMPLIFY = 0.9; // Slightly lower the volume to avoid volume overflow-compression
			var MAX_PEAK_RATIO = 1e-8; // Do not find peaks below this level: 80dB from max
			var MAX_PEAK_JUMP = (Math.pow(2.0,50/1200.0)-1.0); // Rel distance (in freq) to look for matches
			var MATCH_MAG_THRESH = 0.1; // New if mag_prev < MATCH_MAG_THRESH * mag_new
			
			var windowSize = 1 << wsizeLog;
			var fft = FFT(wsizeLog);

			// Caluclate max step size for both ana and syn windows
			// Has to be < 1/4 of window length or audible artifacts occur
			var max_step_len = 1 << (wsizeLog - 2); // 1/4 of window_size
			max_step_len -= max_step_len % 100; // Change to a multiple of 100 as tempo is often changed in percents

			//console.log("MAX STEP",max_step_len,windowSize);
			var in_buffer = VH.float_array(windowSize + max_step_len + 5);
			var out_buffer = VH.float_array(windowSize + max_step_len + 5);
			var ana_len = max_step_len, syn_len = max_step_len;

			// Hanning window
			var win = VH.float_array(windowSize);
			for(var i=0;i<windowSize;i++)
				win[i] = 0.5 * (1 - Math.cos(2 * Math.PI * i / windowSize));

			var hWS = (windowSize >> 1) + 1;
			var re1 = VH.float_array(hWS), im1 = VH.float_array(hWS);
			var re2 = VH.float_array(hWS), im2 = VH.float_array(hWS);
			var pre2 = VH.float_array(hWS), pim2 = VH.float_array(hWS);

			var qWS = (hWS >> 1) + 1;
			var b_npeaks = [0,0], b_peaks = [], b_in_angs = [], b_peak_adeltas = [];
			var b_mags = [];
			for(var i=0;i<2;i++) { // Double buffering
				b_peaks.push(VH.float_array(qWS));
				b_in_angs.push(VH.float_array(qWS));
				b_peak_adeltas.push(VH.float_array(qWS));
				b_mags.push(VH.float_array(hWS));
			}
			
			var peaks_re = VH.float_array(qWS), peaks_im = VH.float_array(qWS);

			// Keep track of time (in samples) in both input and output streams
			var in_time = 0.0, out_time = 0.0;

			// Track the changes for mapOutputToInputTime
			var changes = [{ in_time: 0.0, out_time: 0.0, tempo: chosen_tempo }];

			var f_ind = 0, prev_out_len = 0, gain_comp = 1.0;
			var syn_drift = 0.0, syn_drift_per_step = 0.0;

			// Two variables used for "process"
			var inbuffer_contains = 0, unused_in_outbuf = 0;

			var obj = { };

			// Should map time in output to time in input
			obj['mapOutputToInputTime'] = function(given_out_time) {
				var ci = changes.length-1;
				while(given_out_time<changes[ci].out_time && ci>0) ci--;
				var cc = changes[ci];
				return cc.in_time + cc.tempo*(given_out_time-cc.out_time);
			};

			obj['flush'] = function(discard_output_seconds) {
				syn_drift = 0.0; b_npeaks = [0,0]; prev_out_len = 0;
				unused_in_outbuf = 0; inbuffer_contains = 0;

				for(var i=0;i<2;i++)
					for(var k=0;k<hWS;k++)
						b_mags[i][k] = 0.0;

				for(var i=0;i<in_buffer.length;i++) in_buffer[i] = 0.0;
				for(var i=0;i<out_buffer.length;i++) out_buffer[i] = 0.0;

				// Scroll time cursor back by discard_output_seconds
				if (discard_output_seconds) {

					// Scroll back time in both coordinates
					out_time = Math.max(0,out_time-discard_output_seconds);
					in_time = obj['mapOutputToInputTime'](out_time);

					// Clear the now-made-future tempo changes (if any)
					var ci = changes.length-1;
					while(out_time<=changes[ci].out_time && ci>=0) { changes.pop(); ci--; }

					// Add a tempo change reflecting current state
					changes.push({ 
						in_time: in_time, out_time: out_time,
						tempo: chosen_tempo
					})
				}
			};

			// Small utility function to calculate gain compensation
			var compute_gain_comp = function(win,syn_len) {
				var n = win.length / syn_len | 0, sum = 0.0;
				for(var i=0;i<n;i++) sum += win[i * syn_len];
				return GAIN_DEAMPLIFY / sum;
			};
			
			obj['getTempo'] = function() { return chosen_tempo; };
			obj['setTempo'] = function(tempo_ratio) {
				ana_len = syn_len = max_step_len;
				if(tempo_ratio >= 1.0) {
					syn_len = Math.round(ana_len / tempo_ratio);
				} else {
					ana_len = Math.round(syn_len * tempo_ratio);
				}
				syn_drift_per_step = (1.0 / tempo_ratio - 1.0 * syn_len / ana_len) * ana_len;
				gain_comp = compute_gain_comp(win,syn_len);
				chosen_tempo = tempo_ratio;
				//console.log("TEMPO CHANGE",tempo_ratio,"LENS",ana_len,syn_len,"GAIN",gain_comp);

				// Handle book-keeping for time map
				var lc = changes[changes.length-1];
				if (lc.out_time == out_time) // no samples since last change
					lc.tempo = tempo_ratio; // Just replace last change event
				else //add new entry
					changes.push({ 
						in_time: in_time, out_time: out_time,
						tempo: tempo_ratio
					})
			};

			obj['flush'](0); obj['setTempo'](chosen_tempo);


			/**************************
			* Small utility functions
			**************************/
			
			// Estimate the phase at (fractional) fft bin ind
			var interpolate_phase = function(re,im,ind) {
				var i = Math.floor(ind);
				var sgn = i % 2 == 1 ? -1.0 : 1.0;
				return Math.atan2(sgn * (im[i] - im[i + 1]),sgn * (re[i] - re[i + 1]));
			};

			// Get ang between -PI and PI
			var unwrap = function(ang) {
				return ang - 2 * Math.PI * Math.round(ang / (2 * Math.PI) );
			};

			// Try to estimate the phase change if window lengths differ by ratio
			var estimate_phase_change = function(ang,k,pang,pk,ratio) {
				var pred = 2 * Math.PI / windowSize * 0.5 * (pk + k) * ana_len;
				var ywang = unwrap(ang - pang - pred);

				return (ywang + pred) * ratio;
			};

			/**************************
			* Find peaks of spectrum
			**************************/

			var find_rpeaks = function(mags,res) {

				var max = 0; for(var i=0;i<mags.length;i++) if (mags[i]>max) max=mags[i];
				var thresh = MAX_PEAK_RATIO * max;

				var n_peaks = 1, prev_pi = 1; res[0] = 1.0;
				for(var i=2;i<mags.length;i++) {
					var f_delta = i * MAX_PEAK_JUMP;
					if(mags[i]>thresh && mags[i] > mags[i - 1] && mags[i] >= mags[i + 1]) { // Is local peak

						// Use quadratic interpolation to fine-tune the peak location
						var ppos = i + (mags[i - 1] - mags[i + 1]) / (2 * (mags[i - 1] - 2 * mags[i] + mags[i + 1]));

						// If far enough from previous peak, add to list
						if(ppos - res[n_peaks - 1] > f_delta) { res[n_peaks++] = ppos; prev_pi = i; }
						// Else, if not far enough, but higher than previous, just replace prev 
						else if(mags[i] > mags[prev_pi]) { res[n_peaks - 1] = ppos;	prev_pi = i; }
					}
				}
				return n_peaks;
			};

			/**************************
			* Rigid phase shift
			**************************/

			var pshift_rigid = function(frame_ind,re,im,p_re,p_im,ratio) {
				var CUR = frame_ind % 2, PREV = 1 - CUR;

				var prev_mags = b_mags[PREV];

				var prev_np = b_npeaks[PREV], prpeaks = b_peaks[PREV];
				var prev_in_angs = b_in_angs[PREV], prev_peak_adeltas = b_peak_adeltas[PREV];

				// Calc new mags
				var mags = b_mags[CUR];
				for(var i=1;i<mags.length;i++) mags[i] = re[i] * re[i] + im[i] * im[i];
			
				// Find new peaks
				var peaks = b_peaks[CUR];
				var cur_np = b_npeaks[CUR] = find_rpeaks(mags,peaks);

				// Start adjusting angles
				var cur_in_angs = b_in_angs[CUR], cur_peak_adeltas = b_peak_adeltas[CUR];

				if(frame_ind == 0 || cur_np == 0) { // If first frame (or no peaks)

					// Set out_ang = in_ang for all peaks
					for(var ci=0;ci<cur_np;ci++) {
						var pci = peaks[ci];
						prev_in_angs[ci] = prev_peak_adeltas[ci] = interpolate_phase(re,im,pci);
					}
					
					return;
				}

			    /*********************************************************
		    	* Match old peaks with new ones
		    	* Also find where pmag*mag is max for next step
		    	*********************************************************/

				var pi = 0;
				for(var ci=0;ci<cur_np;ci++) {
					var pci = peaks[ci];

					// Scroll so peaks[ci] is between prpeaks[pi] and prpeaks[pi+1]
					while(peaks[ci] > prpeaks[pi] && pi != prev_np) ++pi;

					var cpi = pi;
					if(pi > 0 && pci - prpeaks[pi - 1] < prpeaks[pi] - pci) cpi = pi - 1;

					var peak_delta = pci * MAX_PEAK_JUMP;
					if(Math.abs(prpeaks[cpi] - pci) < peak_delta && 
						prev_mags[Math.round(prpeaks[cpi])] > 
							MATCH_MAG_THRESH * mags[Math.round(pci)]) {

						// Found a matching peak in previous frame, so predict based on the diff
						var in_angle = interpolate_phase(re,im,pci);
						var out_angle = prev_in_angs[cpi] + prev_peak_adeltas[cpi] +
								estimate_phase_change(in_angle,pci,prev_in_angs[cpi],prpeaks[cpi],ratio);

						var delta = out_angle - in_angle;
						cur_in_angs[ci] = in_angle; cur_peak_adeltas[ci] = delta;
						peaks_re[ci] = Math.cos(delta);	peaks_im[ci] = Math.sin(delta);
					} else { // Not matched - use the same phase as input
						cur_in_angs[ci] = interpolate_phase(re,im,pci);
						cur_peak_adeltas[ci] = 0; peaks_re[ci] = 1.0;	peaks_im[ci] = 0.0;				
					}
				}

			    /********************************************************
			    * Adjust phase of all bins based on closest peak
			    *********************************************************/

			    // Add a "dummy" peak at the end of array
				peaks[cur_np] = 2 * windowSize;
				
				var cpi = 0, cp = peaks[cpi], cnp = peaks[cpi + 1];
				var cre = peaks_re[cpi], cim = peaks_im[cpi];

				for(var i=1;i<re.length-1;i++) {
					if(i >= cp && i - cp > cnp - i) {
						++cpi; cp = peaks[cpi];	cnp = peaks[cpi + 1];
						cre = peaks_re[cpi]; cim = peaks_im[cpi];
					}

					var nre = re[i] * cre - im[i] * cim;
					var nim = re[i] * cim + im[i] * cre;
					re[i] = nre; im[i] = nim;
				}
			}

			/***********************************
			* Perform two syn/ana steps 
			*	(using the two-for-one fft trick)
		  	* Takes windowSize + ana_len samples from in_buffer
		  	*   and shifts in_buffer back by 2*ana_len
		  	* Outputs <retval> samples to out_buffer
			***********************************/

			var two_steps = function() {

				// To better match the given ratio,
		    	// occasionally tweak syn_len by 1 or 2
				syn_drift += 2 * syn_drift_per_step;
				var sdelta = syn_drift | 0;
				syn_drift -= sdelta;
				
				// Pack two steps into fft object
				for(var i=0;i<windowSize;i++) {
					fft.m_re[i] = win[i] * in_buffer[i];
					fft.m_im[i] = win[i] * in_buffer[ana_len + i];
				}

				// Shift in_buffer back by 2*ana_len
				VH.blit(in_buffer,2*ana_len,
		            in_buffer,0,windowSize-ana_len);

				// Run the fft
				fft.inplace(false);
				fft.unpack(re1,im1,re2,im2);

				// Step 1 - move by syn_len
				var ratio1 = 1.0 * syn_len / ana_len;
				pshift_rigid(f_ind,re1,im1,pre2,pim2,ratio1);

				// Step 2 - move by syn_len+sdelta
				var ratio2 = 1.0 * (syn_len + sdelta) / ana_len;
				pshift_rigid(f_ind + 1,re2,im2,re1,im1,ratio2);

				// Save (modified) re and im
				VH.blit(re2,0,pre2,0,hWS); VH.blit(im2,0,pim2,0,hWS);

				// Run ifft
				fft.repack(re1,im1,re2,im2);
				fft.inplace(true);

				// Shift out_buffer back by previous out_len;
				var oblen = out_buffer.length;
				VH.blit(out_buffer,prev_out_len,
		            out_buffer,0,oblen-prev_out_len);
				
				// And shift in zeros at the end
				for(var i=oblen-prev_out_len;i<oblen;i++) out_buffer[i] = 0.0;
				
				// Value overflow protection - scale the packet if max above a threshold
			    // The distortion this creates is insignificant compared to phase issues
				var max = 0.0, gc = gain_comp;
				for(var i=0;i<syn_len;i++)
					if(Math.abs(2 * fft.m_re[i]) > max)
						max = Math.abs(2 * fft.m_re[i]);
				for(var i=0;i<windowSize-syn_len;i++)
					if(Math.abs(fft.m_re[i + syn_len + sdelta] + fft.m_im[i]) > max)
						max = Math.abs(fft.m_re[i + syn_len + sdelta] + fft.m_im[i]);

				for(var i=windowSize-syn_len;i<windowSize;i++)
					if(Math.abs(2 * fft.m_im[i]) > max)
						max = Math.abs(2 * fft.m_im[i]);

				// Find allowed ceiling of a two-step sum and lower gain if needed
				var ceiling = 1.0 / Math.floor(1.0 * windowSize / (2 * syn_len));
				if(gc * max > ceiling) {
					//console.log("Gain overflow, lowering volume: ",ceiling / max,gc,max);
					gc = ceiling / max;
				}

				// Write results to out_buffer
				for(var i=0;i<windowSize;i++) {
					out_buffer[i] += gc * fft.m_re[i];
					out_buffer[i + syn_len + sdelta] += gc * fft.m_im[i];
				}

				f_ind += 2;	prev_out_len = 2 * syn_len + sdelta;

				return prev_out_len;
			}

			// input: array of channels, each a float_array with unbounded amount of samples
			// output: same format
			obj['process'] = function(in_ar) {

				var in_len = in_ar[0].length;

				// Mix channels together (if needed)
				var mix = in_ar[0]; 
				if (in_ar.length>1) {
					mix = VH.float_array(in_ar[0].length);
					var mult = 1.0/in_ar.length;
					for(var c=0;c<in_ar.length;c++)
						for(var i=0;i<in_len;i++)
							mix[i] += mult*in_ar[c][i];
				}

				// Handle the special case of no tempo change
				if (chosen_tempo == 1.0) {

					// Empty out_buffer followed by in_buffer, if they are not empty
					if (unused_in_outbuf+inbuffer_contains>0) {
						var n_len = unused_in_outbuf + inbuffer_contains + in_len;
						var n_ar = [];
						for(var c=0;c<in_ar.length;c++) { 
							var buf = VH.float_array(n_len);
							VH.blit(out_buffer,0,buf,0,unused_in_outbuf);
							VH.blit(in_buffer,0,buf,unused_in_outbuf,inbuffer_contains);
							VH.blit(in_ar[c],0,buf,unused_in_outbuf+inbuffer_contains,in_len);
							n_ar.push(buf);
						}
						obj['flush'](0);
						in_len = n_len; in_ar = n_ar;
					}

					// Move time pointers
					in_time += in_len/sampleRate; out_time += in_len/sampleRate;

					// Just return the same samples as were given as input
					return in_ar;
				}

				// Calculate output length
				// Should underestimate, and by no more than 4, which can easily fit in the unused_in_outbuf
				var consumable_samples = inbuffer_contains + in_len - (windowSize - ana_len);
				var n_steps = 2*Math.floor(Math.max(0,consumable_samples)/(2*ana_len));
				var out_len = unused_in_outbuf + syn_len*n_steps +
								Math.floor(syn_drift+syn_drift_per_step*n_steps);

				if (unused_in_outbuf>out_len) out_len = unused_in_outbuf;

				// Allocate output
				var outp = VH.float_array(out_len);

				// Copy previously unused but ready values to output
				VH.blit(out_buffer,0,outp,0,unused_in_outbuf); 
				var ii = 0, oi = unused_in_outbuf;
				
				var left_over = 0, res_len = 0;
				while(true) {

					// Calculate how many new samples we need to call two_steps
					var n_needed = windowSize + ana_len - inbuffer_contains;
					
					if (ii+n_needed>in_len) { // Not enough samples for next step
						// Copy whats left to inbuffer and break out of the loop
						VH.blit(mix,ii,in_buffer,inbuffer_contains,in_len-ii);
						inbuffer_contains += in_len-ii; ii = in_len;
						break;
					}
					else if (n_needed <= 0) // Already enough - can happen if tempo changed
						inbuffer_contains -= 2 * ana_len; 
					else { // Main case - we have enough
						// Copy over this many samples from input
						VH.blit(mix,ii,in_buffer,inbuffer_contains,n_needed);
						ii += n_needed;					
						inbuffer_contains = windowSize - ana_len;
					}

					// Invariant: left_over should be 0 here as it should break!

					// Run the vocoder
					res_len = two_steps();

					// Move time pointers
					in_time += 2*ana_len/sampleRate; out_time += res_len/sampleRate;

					// Calculate how many samples are left over (usually 0)
					left_over = oi + res_len - out_len; if(left_over < 0) left_over = 0;

					// Copy fully ready samples out
			        VH.blit(out_buffer,0,outp,oi,res_len-left_over);

					oi += res_len;
				}

				// Copy left over samples to the beginning of out_buffer
	  			VH.blit(out_buffer,res_len-left_over,out_buffer,0,left_over);
	  			unused_in_outbuf = left_over;

	  			//////////////////////// DONE

				// Clone the result to match the number of input channels
				var out_ar = [];
				for(var c=0;c<in_ar.length;c++) out_ar.push(outp);

				return out_ar;
			};

			return obj;
		};

		/** @export */
		module.exports = AudioTempoChanger;
	})();


	/***/ }),
	/* 2 */
	/***/ (function(module, exports, __webpack_require__) {

	"use strict";


	/*
	 * Performs an in-place complex FFT.
	 * Adapted from FFT for ActionScript 3 written by Gerald T. Beauregard 
	 * (original ActionScript3 version, http://gerrybeauregard.wordpress.com/2010/08/03/an-even-faster-as3-fft/)
	 *
	 * Copyright (c) 2015-2019 Margus Niitsoo
	 */

	var VH = __webpack_require__(0);

	var FFT = function(logN) {

		// Size of the buffer
		var m_N = 1 << logN;


		var obj = {
			m_logN : logN, m_N : m_N,
			m_invN : 1.0 / m_N,
			m_re : VH.float_array(m_N),
			m_im : VH.float_array(m_N),
			m_revTgt : new Array(m_N)
		}

		// Calculate bit reversals
		for(var k = 0; k<m_N; k++) {
			var x = k, y = 0;
			for(var i=0;i<logN;i++) {
				y <<= 1;
				y |= x & 1;
				x >>= 1;
			}
			obj.m_revTgt[k] = y;
		}

	    // Compute a multiplier factor for the "twiddle factors".
	    // The twiddle factors are complex unit vectors spaced at
	    // regular angular intervals. The angle by which the twiddle
	    // factor advances depends on the FFT stage. In many FFT
	    // implementations the twiddle factors are cached.

		obj.twiddleRe = VH.float_array(obj.m_logN);
		obj.twiddleIm = VH.float_array(obj.m_logN);

		var wIndexStep = 1;
		for(var stage = 0; stage<obj.m_logN; stage++) {
			var wAngleInc = 2.0 * wIndexStep * Math.PI * obj.m_invN;
			obj.twiddleRe[stage] = Math.cos(wAngleInc);
			obj.twiddleIm[stage] = Math.sin(wAngleInc);
			wIndexStep <<= 1;
		}

		// In-place FFT function
		obj.inplace = function(inverse) {

			var m_re = obj.m_re, m_im = obj.m_im;
			var m_N = obj.m_N, m_logN = obj.m_logN;

			var numFlies = m_N >> 1;
			var span = m_N >> 1;
			var spacing = m_N;

			if(inverse) {
				var m_invN = 1.0/m_N;
				for(var i=0; i<m_N; i++) {
					m_re[i] *= m_invN;
					m_im[i] *= m_invN;
				}
			}

			// For each stage of the FFT
			for(var stage=0; stage<m_logN; stage++) {
				var wMulRe = obj.twiddleRe[stage];
				var wMulIm = obj.twiddleIm[stage];
				if(!inverse) wMulIm *= -1;

				var start = 0;
				while(start < m_N) {
					var iTop = start, iBot = start + span;
					var wRe = 1.0, wIm = 0.0;

					// For each butterfly in this stage
					for(var flyCount=0; flyCount<numFlies; flyCount++) {
						// Get the top & bottom values
						var xTopRe = m_re[iTop];
						var xTopIm = m_im[iTop];
						var xBotRe = m_re[iBot];
						var xBotIm = m_im[iBot];

						// Top branch of butterfly has addition
						m_re[iTop] = xTopRe + xBotRe;
						m_im[iTop] = xTopIm + xBotIm;

						// Bottom branch of butterly has subtraction,
	                    // followed by multiplication by twiddle factor
						xBotRe = xTopRe - xBotRe;
						xBotIm = xTopIm - xBotIm;

						m_re[iBot] = xBotRe * wRe - xBotIm * wIm;
						m_im[iBot] = xBotRe * wIm + xBotIm * wRe;

						// Advance butterfly to next top & bottom positions
	                    iTop++;
	                    iBot++;

	                    // Update the twiddle factor, via complex multiply
	                    // by unit vector with the appropriate angle
	                    // (wRe + j wIm) = (wRe + j wIm) x (wMulRe + j wMulIm)
						var tRe = wRe;
						wRe = wRe * wMulRe - wIm * wMulIm;
						wIm = tRe * wMulIm + wIm * wMulRe;
					}
					start += spacing;
				}
				numFlies >>= 1;
				span >>= 1;
				spacing >>= 1;
			}

			var revI, buf, m_revTgt = obj.m_revTgt;
			for(var i1=0; i1<m_N; i1++)
				if(m_revTgt[i1] > i1) {
	                // Bit-Reversal is an involution i.e.
	                // x.revTgt.revTgt==x
	                // So switching values around
	                // restores the original order
					revI = m_revTgt[i1];
					buf = m_re[revI];
					m_re[revI] = m_re[i1];
					m_re[i1] = buf;
					buf = m_im[revI];
					m_im[revI] = m_im[i1];
					m_im[i1] = buf;
				}
		}

		var m_N2 = m_N >> 1; // m_N/2 needed in un/repack below

		// Two-for-one trick for real-valued FFT:
		// Put one series in re, other in im, run "inplace",
		// then call this "unpack" function
		obj.unpack = function(rre,rim,ire,iim) {
			rre[0] = obj.m_re[0]; ire[0] = obj.m_im[0];
			rim[0] = iim[0] = 0;
			rre[m_N2] = obj.m_re[m_N2];
			ire[m_N2] = obj.m_im[m_N2];
			rim[m_N2] = iim[m_N2] = 0;
			for(var i = 1;i<m_N2;i++) {
				rre[i] = (obj.m_re[i] + obj.m_re[m_N - i]) / 2;
				rim[i] = (obj.m_im[i] - obj.m_im[m_N - i]) / 2;
				ire[i] = (obj.m_im[i] + obj.m_im[m_N - i]) / 2;
				iim[i] = (-obj.m_re[i] + obj.m_re[m_N - i]) / 2;
			}
		}
		
		// The two-for-one trick if you know results are real-valued
		// Call "repack", then fft.inplace(true) and you have
		// First fft in re and second in im
		obj.repack = function(rre,rim,ire,iim) {
			obj.m_re[0] = rre[0]; obj.m_im[0] = ire[0];
			obj.m_re[m_N2] = rre[m_N2]; obj.m_im[m_N2] = ire[m_N2];
			for(var i = 1;i<m_N2;i++) {
				obj.m_re[i] = rre[i] - iim[i];
				obj.m_im[i] = rim[i] + ire[i];
				obj.m_re[m_N - i] = rre[i] + iim[i];
				obj.m_im[m_N - i] = -rim[i] + ire[i];
			}
		}

		return obj;
	};

	module.exports = FFT;

	/***/ })
	/******/ ]);
	});
	//# sourceMappingURL=AudioTempoChanger.js.map

/***/ })
/******/ ])
});
;